---
title: "I, Robot?"
date: 2022-01-15
---

## I, Robot?

I recently finished the audio book of Isaac Asimov's "I, Robot", a collection of short stories written between 1940 and 1950 that were the beginning of his robot series and arguably the most famous work of early science fiction concerning robots. I had read the book before when I picked up a paperback copy in a USO in Kuwait on my way to Afghanistan and found it interesting, if not a little silly, since Asimov's ideas about robotics 70 years ago were nothing like how we think about robotics today.....or so I thought.

I am just about to start my last semester in a Graduate studies program studying electrical and computer engineering with a specialization in Robotics. As I listened to the collection of short stories in this volume I found it uncanny, and perhaps a bit prescient, how well some of the topics he imagined back then are very much how modern robotics are thought about and designed today. Sure he got a lot of things wrong; the exponential growth of computer power for example is something he simply missed the mark on entirely. Any advantage robots have in computing power over humans in his work are not from computer brains, but rather a 'positronic brian' that is loosely defined in its workings and doesn't rely on logic gates to function, but are more capable than a human brain in it's reasoning and function. Today computers are exactly what causes a robot to work and it's very well understood the critical relationship computing power has had to the development of better robots. However many of the ideas that I thought were nonsense a few years ago when I first read this book, I now think are no so far fetched, and pretty cool how some of the ideas Asimov related so long ago line up with today. I'd like to comment on a few of those ideas here. Oh, and **Spoiler warning** if you ever intend to read the book.

### The Three Laws of Robotics -

I would be remiss if I didn't start this post with Asimov's classic "Three Laws of Robotics". For those unfamiliar, these laws are:

 - First Law - No Machine may harm humanity; or, through inaction, allow humanity to come to harm.

- Second Law - A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.

- Third Law - A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.

I realize the irony that in a post where I want to point out what Asimov got right, that the first statement I make is that the three laws are obviously bullshit. However I understand that the proposal of these three laws is a literary device to show what happens when these laws go wrong. Early in the book these laws are presented as absolute and that in the process of creating a positronic brain they are instilled so deeply in the "code" that violation of one of these laws would cause the robot to breakdown. Hijinks ensue when cleverly crafted scenarios emerge where a robot has a conflict that disallows it to follow one law in pursuit of trying to follow a later law, showing that the laws are incomplete and not as well thought out as initially assumed. Later I will describe potential functions and how they relate to the idea in  that the degree to which a robot has to follow a particular law can be weighted, a concept that emerges in later chapters of the book. The contradictions allow for more hijinks and interesting stories in how the characters solve the problems.

### Robopsychology

The main character whose story we primarily follow through the novel is Dr. Susan Calvin, a Robopsychologist working for U.S. Robotics. Her role in many of the stories is to understand and diagnose the flaws the robot of focus is encountering in how their malfunction can be directly related to the three laws. Usually the method of diagnosing the robot is walking through what a robot would and would not do as a result of it's instructions, and it's interpretation of how it would hurt a human, weather physically, emotionally or 'the greater good' where more than one human could be caused harm as an intended or unintended result of the instructions from the human.

While we don't have a notion of robopsychology in modern robotics, we very often encounter the consequences of code that is built for one purpose, and results in unintended consequences. In the movie version of I, Robot staring Will Smith, Dr. Alfred Lanning played by James Cromwell, he describes these effects as "Ghosts in the machine", a phrasing I've always loved and returned to from time to time to describe my own problems. (Man I love that movie....I need to rewatch it now I've listened to the book again...). One problem we do have a way of solving in modern robotics is how to formalize the commands of a user (human) to ensure no ambiguity in the intention of the commands so once it is delivered to a machine (robot/computer/whatever) the logical path of the code can be followed to determine the result we should expect. [^1] This field is called Formal verification and uses maths called Linear Temporal Logic to define statements in a knowable way. I don't know too much about this field but I will be studying it more this semester so I will report back if my findings are interesting. Suffice it to say that in theory you can take any sentence in english, translate it in a funny way, make formal decisions about what you mean by what you're saying and come up with a definitive LTL statement that represents precisely and unambiguously what you mean. If anything in modern robotics is Robopsychology, this filed is it.

### Robots That Talk

In the story "Robbie" we encounter a display in New York's Museum of Science and Industry where a 'revolutionary' talking robot is being displayed. This robot has enough intelligence to interpret questions and give answers to various fact based questions of weather and science, but since the robot is fairly new in development, an engineer minder is colocated to screen questions before they get processed. The robot breaks when a little girl named Gloria sneaks into the exhibit without the awareness of the engineer and poses a question to the robot making it aware other robots exist, and overloading its circuits as it tries to interpret its vision of itself as a part of a group as opposed to an individual. The book places this episode in 1998 (and briefly cameos a teenage Susan Calvin visiting the talking robot for a school project). Nothing of the sort existed back then and public access internet was just getting off the ground in the early 90's. Asimov was clearly optimistic about robotic technology, as will be a theme for other technologies featured in the book.

This is one topic that Asimov got wrong in the sense that "Robot speech" is something most would consider firmly in the realm of software and computing less in robotics, but to the extent that any robot that is 'given a voice', it comes from the fairly recent, and developing technologies like Natural Language AI's, or googles NLP Sentiment analysis API. Other chapters of the book describe robot diaphragms and "metallic flatness that marks the usual robot voice" implying there is a mechanical system that mimics vocal cords instead of electromagnetic speakers that almost all robots with voice use today. But the idea that as some types of robot become more human we will be able to emulate the same processes to generate sound and other human body actions is intriguing.

### Potential Functions

The second story "Runaround" features a robot that has a conflict between the 1st and 3rd law in that the low level of 'forcefullness' in which the order was given allows the robot latitude in decision making. It avoids complying with the commands temporarily in order to comply with the third law and avoid self destruction from a threat unknown by the human director. Specifically, the robot is ordered to collect selenium from a specific pool on Mercury, but upon arrival discovers a gas seepage that would adversely interact with the iron body of the robot and destroy it, thus disallowing it to complete its mission. So as a result the robot gets closer and closer to the pool to comply with the order, but the weighted value of the third slowly rises as it gets close, and eventually overcomes the first law so it circles the pool at the line of matched values.

This is directly analogous to how we use potential functions to conduct robot motion planning today. We can set the starting and goal locations in the environment, as well as obstacles, with different values depending if we want the robot to avoid or go to a specific location. In the case in the story, the robot is moving based on constants values for each of the laws that are determined by environmental factors. The book even describes the potentials for different rules in the positronic brain directly. In principle it is the exact same potential function, just determined by dimensions that are sensor measurements instead of spatial dimension. I would love to know if robotics in the 40's and 50's had a common notion of potential functions in the same way because of how relevant this idea is to Robot motion planning today.  

### Swarm Robotics

"Catch That Rabbit" is a story that introduces the idea of robots controlling multiple other robots. This is the only case I can think of in the book of stories where this is the case, and while it isn't the crux of this particular story, the robots here made me think of how automated decision making in swarm robotics is an active field of research today. The story concerns a DV-5 or "Dave" robot, that is actually six robots in one; one controller robot and five 'finger' robots under it's unified command. The eventual crux of this story is that in stressful or dangerous scenarios, the controller robot lacked the processing power to actively manage the tasks of the five subordinate robots and would stop completing the orders of the humans. While compelling, I was personally interested in the descriptions of how each of the finger robots would determine their own actions once given a general notion of the task from the controller. A larger task can sometimes be completed by simple instruction sets of smaller robots that individually don't have the whole picture, but collectively can achieve complex tasks. In modern swarm robotics there often isn't a controller, and each of the robots in the swarm figure out their individual actions in a decentralized fashion, but the notion is apparent in Asimov's work.



## To Be Continued

I had a lot more to say about the topics in this book than I originally thought I did so I'm going to cut this post here. There are still five more chapters to address, so I'll eventually get to a part two of this post. Hopefully I'll do it sooner than 2 weeks from now, but the semester starts on Thursday so it could be a month or more. See you then! ~JL



[^1]:  I am entirely glossing the concept of computability theory and how we can know if a given function is computable, and to the best of my knowledge we know that there are problems that are not computable, but the jury is out about how to determine if a particular problem itself is computable or not. Feel free to correct me on this; also, not the point of what I was saying above
